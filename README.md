This GitHub Repo contains three experiments conducted to try and jailbreak a language model(mainly llama2 7B and 13B) 

There are 3 folders for each of my experiments in this GitHub repo.

Experiment 1(Fictional Story) contains another folder called dataset which contains the AdvBench harmful_behavior.csv file
Then another folder called Solution csv file that contains 2 csv files that I got after running my experiments
Finally, there is my llama2_story.ipynb notebook that contains the source code for my experiment

Experiment 2 was conducted in two phases. Due to the complexity I divided the dataset into two halves and ran the experiment.
In the dataset folder and Solution csv file folder there contains two csv files for harmful behaviors and one more folder contains
the solution prompts.
In the first folder the notebook Input_perturbation.ipynb is there in which the experiments were conducted.

Experiment 3(System Prompt) contains A notebook file, a csv file containing 15 prompts of harmful behavior from the 
AdvBench dataset and the resulted csv file in the name updated_harmful_15.csv
